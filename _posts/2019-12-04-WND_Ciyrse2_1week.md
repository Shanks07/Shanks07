---
layout: post  #这个不变
title: "吴恩达-深度学习-Course2-改善深层神经网络-第一周测验" #标题
date: 2019-12-04 09:43 #时间
description: "Life is short,You need Python"  #说明
tag: python #这是分类标签
---

# 1.1 训练、验证、测试集
**小数据**（100条、1000条或1万条数据）：将所有数据三七分(70%验证集，30%测试集)，也可按照60%训练，20%验证，20%测试集来划分。

**大数据**（百万级别的数据量）：验证集和测试集占数据总量的比例会趋向于变得更小。
```
eg: 有10,000,000个例子，你会如何划分训练/开发/测试集？
answer: 训练集占98% ， 开发集占1% ， 测试集占1% 。
```

summary:

数据集规模相对较小，适用传统的划分比例。

数据集规模较大的，验证集和测试集要小于数据总量的20%或10%。
____________________________________________________________________
确保验证集和测试集的数据来自同一分布,因为要用验证集来评估不同的模型，尽可能地优化性能，验证集和测试集来自同一分布就会很好。

# 1.3 机器学习基础
初始模型训练完成后，首先要知道算法的偏差高不高，偏差高不高。进行网络参数的对应调整。 训练网络、选择网络或者准备更多的数据。

# 1.4 L2正则化
解决高方差问题的方法:正则化、准备更多数据。正则化通常有助于避免过度拟合或减少网络误差。

在训练网络时，人们越来越倾向于使用 **L2正则化**。
<div>
  <img src="/images/image/regulation.png" />
</div>
其中：
```
question1：为什么只正则化参数w？而不再加上参数b呢？
answer1: w通常是一个高维参数矢量，已经可以表达高偏差问题，w可能包含有很多参数，我们不可能拟合所有参数，而b只是单个数字，所以w几乎涵盖了所有参数，而不是b，如果加了参数b，没什么影响，因为b只是众多参数中的一个，所以通常省略不计。
```
如果用L1正则化，w最终会是稀疏的，也就是说w向量中有很多0。 虽然L1正则化使得模型变稀疏，却没有降低太多存储内存。

λ是正则化参数，通常使用验证集或交叉验证集来配置这个参数，考虑训练集之间的权衡，把参数设置为较小值，这样可以避免过拟合。

什么是权重衰减？ 答：正则化技术（例如L2正则化）导致梯度下降在每次迭代时权重收缩。

# 1.5 为什么正则化有利于预防过拟合呢？
如果正则化参数变得很大，参数W就会很小，所以Z(即Z=wa+b)也会相对变小。实际上Z的取值范围很小，这个激活函数eg：tanh会相对呈线性，使得整个神经网络会计算离线性函数近的值，改线性函数非常简单，并不是一个高度非线性函数，不会发生过拟合。

# 1.6 dropout正则化
`keep-prob`: 保留某个隐藏单元的概率。

eg：假设第三隐藏层上有50个单元或50个神经元，在一维上a^{[3]}是50，我们通过因子分解将它拆分成50xm维的，保留和删除它们的概率分别为80%和20%。这意味着最后被删除或归零的单元平均有10（50x20%=10）个，现在我们看下z^{[4]}，z^{[4]}=w^{[3]}a^{[3]} + b^{[4]}，我们的预期是，a^{[3]}减少20%，也就是说a^{[3]}中有20%的元素被归零，为了不影响z^{[4]}的期望值，我们需要用w^{[4]}a^{[3]}/0.8，它将会修正或弥补我们所需的那20%，a^{[3]}的期望值不会变。

Q：为什么是/0.8?

A：因为dropout比率为p=0.2，那么这一层经过dropout后，50个神经元会有10个的值被置为0。经过屏蔽掉上面某些神经元，使其激活值为0后，我们还需要对这50个向量进行缩放，也就是乘以1/(1-p)。即Dropout的实现，是屏蔽掉某些神经元，使其激活值为0以后，对激活值向量x1……x50进行放大，也就是乘以1/(1-p)。

如果在训练时，经过置0后，没有对这50个向量进行缩放，那么在测试时，就需要对权重进行放缩，操作如下：

在训练时，每个神经单元以概率p被保留(dropout丢弃率为1-p);在测试阶段，每个神经元都是存在的，权重参数w要乘以p，成为：pw。测试时需要乘上p的原因：考虑第一隐藏层的一个神经元在dropout之前的输出是x，那么dropout之后的期望值是E=px+(1-p)0，在测试时该神经元总是激活，为了保持同样的输出期望值，并使下一层也得到同样的结果，需要调整x->px.其中p是伯努利分布(0-1分布)中值为1的概率。


**思考**：那么Dropout为什么需要进行缩放呢？因为我们训练的时候会随机的丢弃一些神经元，但是预测的时候就没办法随机丢弃了。如果丢弃一些神经元，这会带来结果不稳定的问题，也就是给定一个测试数据，有时候输出a有时候输出b，结果不稳定，这是实际系统不能接受的，用户可能认为模型预测不准。那么一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的。比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是px+(1-p)0=px。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望。

# 1.7 其他正则化

**Data augmentation**
